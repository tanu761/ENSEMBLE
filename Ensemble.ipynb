{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment start"
      ],
      "metadata": {
        "id": "zBFHvuGxgsv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.1\n",
        "\n",
        "ANS..Ensemble Learning is a machine learning technique that combines multiple models (called base learners or weak learners) to build a stronger and more accurate predictive model.\n",
        "\n",
        "The key idea is that instead of relying on a single model, multiple models working together can reduce bias, variance, and improve generalization.\n",
        "\n",
        "Examples: Bagging, Boosting, Stacking.\n",
        "\n",
        "Benefits: Higher accuracy, robustness to noise, better stability."
      ],
      "metadata": {
        "id": "xa_Yb02Wg14E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.2\n",
        "\n",
        "ANS..\n",
        "| Aspect    | Bagging                                                         | Boosting                                                            |\n",
        "| --------- | --------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| Full form | Bootstrap Aggregating                                           | Sequential Boosting                                                 |\n",
        "| Training  | Models trained in parallel on random subsets (with replacement) | Models trained sequentially, each correcting errors of the previous |\n",
        "| Focus     | Reduces variance                                                | Reduces bias                                                        |\n",
        "| Example   | Random Forest                                                   | AdaBoost, Gradient Boosting, XGBoost                                |\n",
        "| Weighting | Equal weight to all models                                      | Higher weight to misclassified instances                            |\n"
      ],
      "metadata": {
        "id": "3NFfxghyhd5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.3\n",
        "\n",
        "ANS..Bootstrap sampling: Random sampling with replacement from the dataset to create multiple training subsets.\n",
        "\n",
        "In Bagging, bootstrap sampling ensures each base learner is trained on slightly different data â†’ introduces diversity.\n",
        "\n",
        "This reduces variance and prevents overfitting. Random Forest uses bootstrap samples to train decision trees independently."
      ],
      "metadata": {
        "id": "pGihkQauhljx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.4\n",
        "\n",
        "ANS..OOB samples: Data points not selected in the bootstrap sample (roughly 37% of data).\n",
        "\n",
        "These can act like a built-in validation set.\n",
        "\n",
        "OOB score: The prediction accuracy computed using OOB samples. It helps evaluate the ensemble without needing an explicit validation set."
      ],
      "metadata": {
        "id": "dLD0PbFphsqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.5\n",
        "\n",
        "ANS..Decision Tree: Feature importance is based on reduction in Gini Impurity or Information Gain at each split. Prone to bias towards dominant features.\n",
        "\n",
        "Random Forest: Aggregates importance across many trees, making it more stable, less biased, and better at capturing complex relationships"
      ],
      "metadata": {
        "id": "v2wYYqyxhzis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.6\n",
        "\n",
        "ANS.."
      ],
      "metadata": {
        "id": "fEYFTCCDiQaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDyDduiXiV2x",
        "outputId": "6da4dc56-fe97-4818-ebbc-4eb7ebe80810"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.7\n",
        "\n",
        "ANS.."
      ],
      "metadata": {
        "id": "ueteb5vFib-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging with Decision Trees\n",
        "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz8AqkyhihTe",
        "outputId": "61e897a2-9ad4-40cb-d5f4-e09324847123"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.8\n",
        "\n",
        "ANS.."
      ],
      "metadata": {
        "id": "jpsLOYetijrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbSZsGJbirsU",
        "outputId": "eec3e680-dd33-424f-dffc-33fa963363f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9596180717279925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.9\n",
        "\n",
        "ANS.."
      ],
      "metadata": {
        "id": "Z3HQW71yiyHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    housing.data, housing.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, bag_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Pvcq5Oi4UN",
        "outputId": "aa80233e-5293-4c6b-e660-6b826195e178"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2572988359842641\n",
            "Random Forest MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES.10\n",
        "\n",
        "ANS..Choice of Method:\n",
        "\n",
        "Bagging (Random Forest) if high variance, stable predictions needed.\n",
        "\n",
        "Boosting (XGBoost/LightGBM) if we want to reduce bias and capture complex patterns.\n",
        "\n",
        "Handling Overfitting:\n",
        "\n",
        "Use cross-validation, regularization (learning_rate in boosting, max_depth tuning).\n",
        "\n",
        "Early stopping for boosting.\n",
        "\n",
        "Selecting Base Models:\n",
        "\n",
        "Decision Trees (common).\n",
        "\n",
        "Could also use Logistic Regression, SVM, or Neural Nets in stacking.\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "k-fold cross-validation, ROC-AUC, precision-recall for imbalanced loan data.\n",
        "\n",
        "Why Ensemble Helps in Real World:\n",
        "\n",
        "Loan default is high-risk, small error is costly.\n",
        "\n",
        "Ensembles improve accuracy, reduce false negatives (missed defaults).\n",
        "\n",
        "More reliable predictions lead to better lending decisions and reduced financial risk."
      ],
      "metadata": {
        "id": "RyB9-e51jFF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT COMPLETE.."
      ],
      "metadata": {
        "id": "uLrYEpw2jRIL"
      }
    }
  ]
}